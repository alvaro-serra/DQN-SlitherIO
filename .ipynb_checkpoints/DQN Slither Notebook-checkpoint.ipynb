{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Slither Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import universe\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import resize\n",
    "from skimage import data\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.color import rgb2gray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There are 12 possible defined actions. The original number of actions are #pixels * 2, meaning we can\n",
    "press any pixel on the screen to define directions and either press space at the same time or not. In order\n",
    "to lower the number of allowed actions and still get a smooth direction control we decide to define 12 regions\n",
    "in the border of the screen where we will press either the center of the region or a random pixel in that region\n",
    "(depending on the strategy taken) so as to move in that direction.\n",
    "\"\"\"\n",
    "##y = 85-386, x = 18-522 --> screen borders\n",
    "topleft = (19,86)\n",
    "bottomright = (522,386)\n",
    "div_x = 5\n",
    "div_y = 3\n",
    "interx = int((bottomright[0]-topleft[0])/div_x)\n",
    "intery = int((bottomright[1]-topleft[1])/div_y)\n",
    "pointers = ([(topleft[0],topleft[1]+intery*i) for i in range(div_y-1,-1,-1)]+\n",
    "           [(topleft[0]+interx*i,topleft[1]) for i in range(1,div_x-1)]+\n",
    "           [(topleft[0]+interx*4,topleft[1]+intery*i) for i in range(div_y)]+\n",
    "           [(topleft[0]+interx*i,topleft[1]+intery*2) for i in range(div_x-2,0,-1)])\n",
    "\n",
    "ACTIONS = 12 # number of valid actions\n",
    "GAMMA = 0.95 # decay rate of past observations\n",
    "OBSERVATION = 1000. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "EPSILON_DECAY = 0.995\n",
    "INITIAL_EPSILON = 1.0 # starting value of epsilon\n",
    "REPLAY_MEMORY = 320000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 0.001\n",
    "KEEP_PROB = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2act(idx):\n",
    "    p = [[('PointerEvent', x+int(interx/2), y+int(intery/2), 0)] for (x,y) in pointers]\n",
    "    return p[idx]\n",
    "\n",
    "def preprocess_obs_ini(obs_n):\n",
    "    color_s_t = obs_n[0]['vision'][85:386,18:522,:]\n",
    "    grey_s_t = rgb2grey(color_s_t)\n",
    "    s_t = resize(grey_s_t, (80,80))\n",
    "    s_t = rescale_intensity(s_t,out_range=(0,255))\n",
    "    s_t = np.stack((s_t, s_t, s_t, s_t), axis=2)\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "    return s_t\n",
    "\n",
    "def preprocess_obs(obs_n):\n",
    "    color_s_t = obs_n[0]['vision'][85:386,18:522,:]\n",
    "    grey_s_t = rgb2grey(color_s_t)\n",
    "    s_t = resize(grey_s_t, (80,80))\n",
    "    s_t = rescale_intensity(s_t,out_range=(0,255))\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], 1) #1x80x80x1\n",
    "    #s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) #TODO IN THE MAIN SECTION\n",
    "    return s_t\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d_1(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,4,4,1], padding = 'SAME')\n",
    "\n",
    "def conv2d_2(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "def conv2d_3(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool_2x2(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConNet = tf.Graph()\n",
    "\n",
    "with ConNet.as_default():\n",
    "    image = tf.placeholder(tf.float32,shape = (None, 80,80,4))\n",
    "    x_image = image\n",
    "    #x_image = tf.reshape(image, [-1,80,80,4])\n",
    "    actions_ref = tf.placeholder(tf.float32, shape = (None,ACTIONS))#not yet known\n",
    "\n",
    "    #5x5 convolution layer, pool 2x2, depth 1 --> depth 32\n",
    "    W_conv1 = weight_variable([8,8,4,32])\n",
    "    b_conv1 = bias_variable([20,20,32])\n",
    "    h_conv1 = tf.nn.relu(conv2d_1(x_image,W_conv1) + b_conv1)\n",
    "    #h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    #5x5 convolution layer, pool 2x2, depth 32 --> depth 64\n",
    "    W_conv2 = weight_variable([4,4,32,64])\n",
    "    b_conv2 = bias_variable([10,10,64])\n",
    "    h_conv2 = tf.nn.relu(conv2d_2(h_conv1, W_conv2) + b_conv2)\n",
    "    #h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    #5x5 convolution layer, pool 2x2, depth 32 --> depth 64\n",
    "    W_conv3 = weight_variable([3,3,64,64])\n",
    "    b_conv3 = bias_variable([10,10,64])\n",
    "    h_conv3 = tf.nn.relu(conv2d_3(h_conv2, W_conv3) + b_conv3)\n",
    "    #h_pool3 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    input_dim = 10*10*64\n",
    "    #Flatten the filtered images in a vector\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, input_dim])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #Fully connected layer of 1024 neurons (activation function: ReLU) with dropout(prob = keep_prob)\n",
    "    W_fc1 = weight_variable([input_dim,512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    h_fc1=tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #Fully connected layer into 2 labels (activation function: Linear)\n",
    "    W_fc2 = weight_variable([512, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "    q_values = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 #real value\n",
    "\n",
    "    #Loss function: cross-entropy with softmax loss; numerically stable way\n",
    "    #loss = tf.losses.mean_squared_error(label,scores)\n",
    "    loss_value = tf.reduce_mean(tf.squared_difference(q_values, actions_ref))\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = scores))\n",
    "\n",
    "    #Optimization algorithm: ADAM, see https://arxiv.org/abs/1412.6980\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-03 13:34:19,795] Making new env: internet.SlitherIO-v0\n",
      "[2018-05-03 13:34:19,810] Writing logs to file: /tmp/universe-18072.log\n",
      "[2018-05-03 13:34:19,842] Ports used: dict_keys([15904, 15905, 5900, 5901, 5902, 5903, 5904, 5905, 15900, 15901, 15902, 15903])\n",
      "[2018-05-03 13:34:19,844] [0] Creating container: image=quay.io/openai/universe.flashgames:0.20.28. Run the same thing by hand as: docker run -p 5906:5900 -p 15906:15900 --cap-add SYS_ADMIN --ipc host --privileged quay.io/openai/universe.flashgames:0.20.28\n",
      "[2018-05-03 13:34:21,136] Remote closed: address=localhost:15906\n",
      "[2018-05-03 13:34:21,138] Remote closed: address=localhost:5906\n",
      "[2018-05-03 13:34:21,141] At least one sockets was closed by the remote. Sleeping 1s...\n",
      "[2018-05-03 13:34:22,147] Remote closed: address=localhost:5906\n",
      "[2018-05-03 13:34:22,149] Remote closed: address=localhost:15906\n",
      "[2018-05-03 13:34:22,160] At least one sockets was closed by the remote. Sleeping 1s...\n",
      "[2018-05-03 13:34:23,162] Using the golang VNC implementation\n",
      "[2018-05-03 13:34:23,164] Using VNCSession arguments: {'start_timeout': 7, 'subsample_level': 2, 'fine_quality_level': 50, 'encoding': 'tight'}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2018-05-03 13:34:23,178] [0] Connecting to environment: vnc://localhost:5906 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15906/viewer/?password=openai\n",
      "[2018-05-03 13:34:43,250] [0:localhost:5906] ntpdate -q -p 8 localhost call timed out after 20.0s; killing the subprocess. This is ok, but you could have more accurate timings by enabling UDP port 123 traffic to your env. (Alternatively, you can try increasing the timeout by setting environment variable UNIVERSE_NTPDATE_TIMEOUT=10.)\n",
      "[2018-05-03 13:34:43,299] [0:localhost:5906] Sending reset for env_id=internet.SlitherIO-v0 fps=60 episode_id=0\n",
      "[2018-05-03 13:35:43,294] [0] Closing rewarder connection\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "1/1 environments have crashed! Most recent error: {'0': 'Rewarder session failed: Lost connection: connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake) (clean=False code=1006)'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56d1f8fb3588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx2act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0maction_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate_p0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_obs_ini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Annee_2017_18/Q2/Pattern_Recognition_And_Machine_Learning/shortProject/universe/universe/wrappers/timer.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc_env.Timer.step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Calculate how much time was spent actually doing work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Annee_2017_18/Q2/Pattern_Recognition_And_Machine_Learning/shortProject/universe/universe/wrappers/render.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Annee_2017_18/Q2/Pattern_Recognition_And_Machine_Learning/shortProject/universe/universe/wrappers/throttle.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0maction_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_substep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# Merge observation, rewards and metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Annee_2017_18/Q2/Pattern_Recognition_And_Machine_Learning/shortProject/universe/universe/wrappers/throttle.py\u001b[0m in \u001b[0;36m_substep\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Submit the action ASAP, before the thread goes to sleep.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mavailable_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'throttle.observation.available_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Annee_2017_18/Q2/Pattern_Recognition_And_Machine_Learning/shortProject/universe/universe/envs/vnc_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_initial_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_err_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvnc_err_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_crashed_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Annee_2017_18/Q2/Pattern_Recognition_And_Machine_Learning/shortProject/universe/universe/envs/vnc_env.py\u001b[0m in \u001b[0;36m_handle_crashed_n\u001b[0;34m(self, info_n)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{} environments have crashed. No error key in info_n: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{} environments have crashed! Most recent error: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_propagate_obs_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_info_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: 1/1 environments have crashed! Most recent error: {'0': 'Rewarder session failed: Lost connection: connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake) (clean=False code=1006)'}"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = ConNet) as sess:\n",
    "    env = gym.make('internet.SlitherIO-v0')\n",
    "    env.configure(remotes=1)  # automatically creates a local docker container\n",
    "    tf.global_variables_initializer().run() #init Q(s,a|w) with random weights\n",
    "    D = deque() #initialize replay memory D\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    OBSERVE = OBSERVATION\n",
    "    t = 0\n",
    "    reward100 = np.zeros(100)\n",
    "    for i_episode in range(2000):\n",
    "        observation_n = env.reset()\n",
    "        while observation_n[0] == None:  # When Slither hasn't started\n",
    "            action = idx2act(0)\n",
    "            action_n = [action for ob in observation_n]\n",
    "            observation_n, reward_n, done_n, info = env.step(action_n)\n",
    "            env.render()\n",
    "        state_p0 = preprocess_obs_ini(observation_n)\n",
    "        rr = 0\n",
    "        done_n = [False]\n",
    "        while not done_n[0]:\n",
    "            action_array = 0\n",
    "            current_loss = 0\n",
    "            #env.render()\n",
    "            if random.random() < epsilon:\n",
    "                idx = int(random.random()*12)\n",
    "                action = idx2act(idx)\n",
    "                action_n = [action for ob in observation_n]\n",
    "                #print(\"------------- Random Action -------------\")\n",
    "            else:\n",
    "                feed = {image: state_p0, keep_prob: KEEP_PROB}\n",
    "                action_array = sess.run(q_values, feed_dict = feed)\n",
    "                actionidx = np.argmax(action_array)\n",
    "            if epsilon > FINAL_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "            \n",
    "            observation_n, reward_n, done_n, info_n = env.step(action_n)\n",
    "            if done_n[0]:# Punish hard when failing\n",
    "                reward_n[0] = -50\n",
    "                observation_n = np.zeros_like(observation)\n",
    "            if observation_n[0] == None:\n",
    "                break;\n",
    "            if not done_n[0]:\n",
    "                rr += reward_n[0]\n",
    "            state_p1 = preprocess_obs(observation_n)\n",
    "            state_p1 = np.append(state_p1, state_p0[:, :, :, :3], axis=3)\n",
    "            D.append((state_p0,action_n,reward_n[0],state_p1,done_n[0]))\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "            if t>BATCH:\n",
    "                minibatch = random.sample(D,BATCH)\n",
    "                inputs = np.zeros((BATCH, state_p0.shape[1],state_p0.shape[2],state_p0.shape[3]))\n",
    "                targets = np.zeros((BATCH,ACTIONS))\n",
    "                for i in range(0,len(minibatch)):\n",
    "                    state_t = minibatch[i][0]            \n",
    "                    action_t = minibatch[i][1]   #This is action index\n",
    "                    reward_t = minibatch[i][2]\n",
    "                    state_t1 = minibatch[i][3]\n",
    "                    terminal = minibatch[i][4]\n",
    "                    inputs[i] = state_t[0]\n",
    "                    #print(\"inputs:\",state_t)\n",
    "                    #print(\"INPUTS:\",inputs[i])\n",
    "                    feed_state_t = {image: state_t, keep_prob: KEEP_PROB}\n",
    "                    feed_state_t1 = {image: state_t1, keep_prob: KEEP_PROB}\n",
    "                    targets[i] = sess.run(q_values, feed_dict = feed_state_t)\n",
    "                    Q_sa = sess.run(q_values, feed_dict = feed_state_t1)\n",
    "                    if terminal:\n",
    "                        targets[i, action_t] = reward_t\n",
    "                    else:\n",
    "                        #print(\"TargetS_i:\", targets[i])\n",
    "                        #print(\"TARGETS_i:\", targets[i,action_t])\n",
    "                        targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "                        #print(\"Targets_i+1\", targets[i])\n",
    "                    feed_train = {image: [inputs[i]], actions_ref: [targets[i]], keep_prob: KEEP_PROB}\n",
    "                    _, current_loss = sess.run([train_step, loss_value], feed_dict = feed_train)\n",
    "            state_p0 = state_p1\n",
    "            t = t+1\n",
    "            monitor = \"\"\n",
    "            if t <= BATCH:\n",
    "                monitor = \"observe\"\n",
    "            elif t > BATCH and t <= BATCH+EXPLORE:\n",
    "                monitor = \"explore\"\n",
    "            else:\n",
    "                monitor = \"train\"\n",
    "            #print(\"EPISODE:\",i_episode,\"/TIMESTEP:\",t,\"/STATE: \",monitor, \\\n",
    "                    #\"/EPSILON:\",epsilon, \"/ACTION: \",action,\"/REWARD: \",rr, \\\n",
    "                    #\"/Q_MAX:\", action_array,\"/MEAN REWARD:\",np.sum(reward100)/100 , \\\n",
    "                    #\"/Loss: \", current_loss)\n",
    "            if done_n[0]:\n",
    "                print(\"Episode {} finished after {} timesteps\".format(i_episode,rr))    \n",
    "                break\n",
    "        reward100[i_episode%100] = rr\n",
    "        #print(\"EPISODE:\",i_episode,\"/TIMESTEP:\",t,\"/STATE: \",monitor, \\\n",
    "                    #\"/EPSILON:\",epsilon, \"/ACTION: \",action,\"/REWARD: \",rr, \\\n",
    "                    #\"/MEAN REWARD:\",np.sum(reward100)/100 , \\\n",
    "                    #\"/Loss: \", current_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
