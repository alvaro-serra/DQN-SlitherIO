{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Slither Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import universe\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import resize\n",
    "from skimage import data\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.color import rgb2gray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There are 12 possible defined actions. The original number of actions are #pixels * 2, meaning we can\n",
    "press any pixel on the screen to define directions and either press space at the same time or not. In order\n",
    "to lower the number of allowed actions and still get a smooth direction control we decide to define 12 regions\n",
    "in the border of the screen where we will press either the center of the region or a random pixel in that region\n",
    "(depending on the strategy taken) so as to move in that direction.\n",
    "\"\"\"\n",
    "##y = 85-386, x = 18-522 --> screen borders\n",
    "topleft = (19,86)\n",
    "bottomright = (522,386)\n",
    "div_x = 5\n",
    "div_y = 3\n",
    "interx = int((bottomright[0]-topleft[0])/div_x)\n",
    "intery = int((bottomright[1]-topleft[1])/div_y)\n",
    "pointers = ([(topleft[0],topleft[1]+intery*i) for i in range(div_y-1,-1,-1)]+\n",
    "           [(topleft[0]+interx*i,topleft[1]) for i in range(1,div_x-1)]+\n",
    "           [(topleft[0]+interx*4,topleft[1]+intery*i) for i in range(div_y)]+\n",
    "           [(topleft[0]+interx*i,topleft[1]+intery*2) for i in range(div_x-2,0,-1)])\n",
    "\n",
    "ACTIONS = 12 # number of valid actions\n",
    "GAMMA = 0.95 # decay rate of past observations\n",
    "OBSERVATION = 1000. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "EPSILON_DECAY = 0.995\n",
    "INITIAL_EPSILON = 1.0 # starting value of epsilon\n",
    "REPLAY_MEMORY = 320000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 0.001\n",
    "KEEP_PROB = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2act(idx):\n",
    "    p = [[('PointerEvent', x+int(interx/2), y+int(intery/2), 0)] for (x,y) in pointers]\n",
    "    return p[idx]\n",
    "\n",
    "def preprocess_obs_ini(obs_n):\n",
    "    color_s_t = obs_n[0]['vision'][85:386,18:522,:]\n",
    "    grey_s_t = rgb2grey(color_s_t)\n",
    "    s_t = resize(grey_s_t, (80,80))\n",
    "    s_t = rescale_intensity(s_t,out_range=(0,255))\n",
    "    s_t = np.stack((s_t, s_t, s_t, s_t), axis=2)\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "    return s_t\n",
    "\n",
    "def preprocess_obs(obs_n):\n",
    "    color_s_t = obs_n[0]['vision'][85:386,18:522,:]\n",
    "    grey_s_t = rgb2grey(color_s_t)\n",
    "    s_t = resize(grey_s_t, (80,80))\n",
    "    s_t = rescale_intensity(s_t,out_range=(0,255))\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], 1) #1x80x80x1\n",
    "    #s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) #TODO IN THE MAIN SECTION\n",
    "    return s_t\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d_1(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,4,4,1], padding = 'SAME')\n",
    "\n",
    "def conv2d_2(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "def conv2d_3(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool_2x2(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConNet = tf.Graph()\n",
    "\n",
    "with ConNet.as_default():\n",
    "    image = tf.placeholder(tf.float32,shape = (None, 80,80,4))\n",
    "    x_image = image\n",
    "    #x_image = tf.reshape(image, [-1,80,80,4])\n",
    "    actions_ref = tf.placeholder(tf.float32, shape = (None,ACTIONS))#not yet known\n",
    "\n",
    "    #5x5 convolution layer, pool 2x2, depth 1 --> depth 32\n",
    "    W_conv1 = weight_variable([8,8,4,32])\n",
    "    b_conv1 = bias_variable([20,20,32])\n",
    "    h_conv1 = tf.nn.relu(conv2d_1(x_image,W_conv1) + b_conv1)\n",
    "    #h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    #5x5 convolution layer, pool 2x2, depth 32 --> depth 64\n",
    "    W_conv2 = weight_variable([4,4,32,64])\n",
    "    b_conv2 = bias_variable([10,10,64])\n",
    "    h_conv2 = tf.nn.relu(conv2d_2(h_conv1, W_conv2) + b_conv2)\n",
    "    #h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    #5x5 convolution layer, pool 2x2, depth 32 --> depth 64\n",
    "    W_conv3 = weight_variable([3,3,64,64])\n",
    "    b_conv3 = bias_variable([10,10,64])\n",
    "    h_conv3 = tf.nn.relu(conv2d_3(h_conv2, W_conv3) + b_conv3)\n",
    "    #h_pool3 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    input_dim = 10*10*64\n",
    "    #Flatten the filtered images in a vector\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, input_dim])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #Fully connected layer of 1024 neurons (activation function: ReLU) with dropout(prob = keep_prob)\n",
    "    W_fc1 = weight_variable([input_dim,512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "    h_fc1=tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #Fully connected layer into 2 labels (activation function: Linear)\n",
    "    W_fc2 = weight_variable([512, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "    q_values = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 #real value\n",
    "\n",
    "    #Loss function: cross-entropy with softmax loss; numerically stable way\n",
    "    #loss = tf.losses.mean_squared_error(label,scores)\n",
    "    loss_value = tf.reduce_mean(tf.squared_difference(q_values, actions_ref))\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = label, logits = scores))\n",
    "\n",
    "    #Optimization algorithm: ADAM, see https://arxiv.org/abs/1412.6980\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-08 12:02:44,708] Making new env: internet.SlitherIO-v0\n",
      "[2018-05-08 12:02:44,722] Writing logs to file: /tmp/universe-28701.log\n",
      "[2018-05-08 12:02:44,740] Ports used: dict_keys([15902, 5900, 15900, 5901, 5902, 15901])\n",
      "[2018-05-08 12:02:44,742] [0] Creating container: image=quay.io/openai/universe.flashgames:0.20.28. Run the same thing by hand as: docker run -p 5903:5900 -p 15903:15900 --cap-add SYS_ADMIN --ipc host --privileged quay.io/openai/universe.flashgames:0.20.28\n",
      "[2018-05-08 12:02:45,861] Remote closed: address=localhost:5903\n",
      "[2018-05-08 12:02:45,863] At least one sockets was closed by the remote. Sleeping 1s...\n",
      "[2018-05-08 12:02:46,868] Remote closed: address=localhost:5903\n",
      "[2018-05-08 12:02:46,873] Remote closed: address=localhost:15903\n",
      "[2018-05-08 12:02:46,877] At least one sockets was closed by the remote. Sleeping 1s...\n",
      "[2018-05-08 12:02:47,881] Using the golang VNC implementation\n",
      "[2018-05-08 12:02:47,882] Using VNCSession arguments: {'fine_quality_level': 50, 'start_timeout': 7, 'encoding': 'tight', 'subsample_level': 2}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2018-05-08 12:02:47,893] [0] Connecting to environment: vnc://localhost:5903 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15903/viewer/?password=openai\n",
      "[2018-05-08 12:03:07,952] [0:localhost:5903] ntpdate -q -p 8 localhost call timed out after 20.0s; killing the subprocess. This is ok, but you could have more accurate timings by enabling UDP port 123 traffic to your env. (Alternatively, you can try increasing the timeout by setting environment variable UNIVERSE_NTPDATE_TIMEOUT=10.)\n",
      "[2018-05-08 12:03:07,994] [0:localhost:5903] Sending reset for env_id=internet.SlitherIO-v0 fps=60 episode_id=0\n",
      "[2018-05-08 12:03:41,855] [0:localhost:5903] Initial reset complete: episode_id=2\n",
      "/home/aserra/tensorflow/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Not Random ----------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = ConNet) as sess:\n",
    "    env = gym.make('internet.SlitherIO-v0')\n",
    "    env.configure(remotes=1)  # automatically creates a local docker container\n",
    "    tf.global_variables_initializer().run() #init Q(s,a|w) with random weights\n",
    "    D = deque() #initialize replay memory D\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    OBSERVE = OBSERVATION\n",
    "    verbose = True\n",
    "    debug = False;\n",
    "    t = 0\n",
    "    reward100 = np.zeros(100)\n",
    "    for i_episode in range(2000):\n",
    "        if debug:\n",
    "            break\n",
    "        if i_episode == 0:\n",
    "            observation_n = env.reset()\n",
    "        while observation_n[0] == None:  # When Slither hasn't started\n",
    "            action = idx2act(0)\n",
    "            action_n = [action for ob in observation_n]\n",
    "            observation_n, reward_n, done_n, info = env.step(action_n)\n",
    "            env.render()\n",
    "        state_p0 = preprocess_obs_ini(observation_n)\n",
    "        rr = 0\n",
    "        done_n = [False]\n",
    "        echo_ts = 0\n",
    "        while not done_n[0]:\n",
    "            action_array = 0\n",
    "            current_loss = 0\n",
    "            env.render()\n",
    "            echo_ts += 1\n",
    "            t+=1\n",
    "            #if echo_ts%10 == 0 and verbose:\n",
    "            #    print(echo_ts)\n",
    "            if random.random() < epsilon:\n",
    "                actionidx = int(random.random()*12)\n",
    "                action = idx2act(actionidx)\n",
    "                action_n = [action for ob in observation_n]\n",
    "                print(\"------------- Random Action -------------\")\n",
    "            else:\n",
    "                feed = {image: state_p0, keep_prob: KEEP_PROB}\n",
    "                #print(\"------------- Not Random ----------------\")\n",
    "                action_array = sess.run(q_values, feed_dict = feed)\n",
    "                if np.isnan(action_array[0][0]):\n",
    "                    print(\"NAN IN THE HOLE!!!!\")\n",
    "                    debug = True\n",
    "                actionidx = np.argmax(action_array)\n",
    "                action = idx2act(actionidx)\n",
    "                action_n = [action for ob in observation_n]\n",
    "            if epsilon > FINAL_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "            \n",
    "            observation_n, reward_n, done_n, info_n = env.step(action_n)\n",
    "            if done_n[0]:# Punish hard when failing\n",
    "                reward_n[0] = -50\n",
    "                #print(state_p0)\n",
    "                state_p1 = np.zeros_like(state_p0)\n",
    "                #print('FINALLY OVER!!!')\n",
    "                #print(state_p1)\n",
    "                D.append((state_p0,actionidx,reward_n[0],state_p1,done_n[0]))\n",
    "\n",
    "            if not done_n[0]:\n",
    "                rr += reward_n[0]\n",
    "                state_p1 = preprocess_obs(observation_n)\n",
    "                state_p1 = np.append(state_p1, state_p0[:, :, :, :3], axis=3)\n",
    "                D.append((state_p0,actionidx,reward_n[0],state_p1,done_n[0]))\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "            if t>BATCH:\n",
    "                minibatch = random.sample(D,BATCH)\n",
    "                inputs = np.zeros((BATCH, state_p0.shape[1],state_p0.shape[2],state_p0.shape[3]))\n",
    "                targets = np.zeros((BATCH,ACTIONS))\n",
    "                for i in range(0,len(minibatch)):\n",
    "                    state_t = minibatch[i][0]            \n",
    "                    action_t = minibatch[i][1]   #This is action index\n",
    "                    reward_t = minibatch[i][2]\n",
    "                    state_t1 = minibatch[i][3]\n",
    "                    terminal = minibatch[i][4]\n",
    "                    inputs[i] = state_t[0]\n",
    "                    #print(\"inputs:\",state_t)\n",
    "                    #print(\"INPUTS:\",inputs[i])\n",
    "                    feed_state_t = {image: state_t, keep_prob: KEEP_PROB}\n",
    "                    feed_state_t1 = {image: state_t1, keep_prob: KEEP_PROB}\n",
    "                    targets[i] = sess.run(q_values, feed_dict = feed_state_t)\n",
    "                    Q_sa = sess.run(q_values, feed_dict = feed_state_t1)\n",
    "                    if terminal:\n",
    "                        targets[i, action_t] = reward_t\n",
    "                    else:\n",
    "                        targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "                    feed_train = {image: [inputs[i]], actions_ref: [targets[i]], keep_prob: KEEP_PROB}\n",
    "                    _, current_loss = sess.run([train_step, loss_value], feed_dict = feed_train)\n",
    "            if done_n[0]:\n",
    "                break;\n",
    "            state_p0 = state_p1\n",
    "            monitor = \"\"\n",
    "            if t <= BATCH:\n",
    "                monitor = \"observe\"\n",
    "            elif t > BATCH and t <= BATCH+EXPLORE:\n",
    "                monitor = \"explore\"\n",
    "            else:\n",
    "                monitor = \"train\"\n",
    "            if verbose and (t%30 == 0 or done_n[0]) :\n",
    "                print(\"EPISODE:\",i_episode,\"/TOTAL TIMESTEP:\",t,\"/TIMESTEP:\",echo_ts,\"/STATE: \",monitor, \\\n",
    "                        \"/EPSILON:\",epsilon, \"/ACTION: \",action,\"/REWARD: \",rr, \\\n",
    "                        \"/Q_MAX:\", action_array,\"/MEAN REWARD:\",np.sum(reward100)/100 , \\\n",
    "                        \"/Loss: \", current_loss)\n",
    "        print(\"Episode {} finished after {} timesteps with score {}\".format(i_episode,echo_ts,rr))    \n",
    "        reward100[i_episode%100] = rr\n",
    "        #if verbose:\n",
    "        #    print(\"EPISODE:\",i_episode,\"/TIMESTEP:\",t,\"/STATE: \",monitor, \\\n",
    "        #                \"/EPSILON:\",epsilon, \"/ACTION: \",action,\"/REWARD: \",rr, \\\n",
    "        #                \"/MEAN REWARD:\",np.sum(reward100)/100 , \\\n",
    "        #                \"/Loss: \", current_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
